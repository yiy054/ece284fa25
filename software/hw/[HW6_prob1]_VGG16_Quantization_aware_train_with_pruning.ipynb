{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-fifty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from tensorboardX import SummaryWriter      \n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import *\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('=> Building model...')\n",
    "    \n",
    "    \n",
    "batch_size = 128\n",
    "model_name = \"VGG16_quant\"\n",
    "model = VGG16_quant()\n",
    "print(model)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "            \n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "         \n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [150, 225]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1        \n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "#all_params = checkpoint['state_dict']\n",
    "#model.load_state_dict(all_params, strict=False)\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "#validate(testloader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-queensland",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"result/VGG16_quant/model_best.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\") \n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8571f14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Unstructured pruning experiment ---\n",
    "import copy\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# helper: apply global unstructured pruning to all conv weights to target sparsity\n",
    "def apply_unstructured_pruning(model, sparsity=0.8):\n",
    "    conv_modules = []\n",
    "    for m in model.modules():\n",
    "        # support QuantConv2d or nn.Conv2d (QuantConv2d should expose weight)\n",
    "        if m.__class__.__name__.endswith('Conv2d') or m.__class__.__name__ == 'QuantConv2d':\n",
    "            if hasattr(m, 'weight') and m.weight is not None:\n",
    "                conv_modules.append((m, 'weight'))\n",
    "    # apply global unstructured pruning\n",
    "    prune.global_unstructured(conv_modules, pruning_method=prune.L1Unstructured, amount=sparsity)\n",
    "    return model\n",
    "\n",
    "# reload original trained checkpoint (4-bit quantized) to keep a clean starting point\n",
    "orig_checkpoint = torch.load(PATH, map_location=device)\n",
    "orig_state = orig_checkpoint['state_dict']\n",
    "base_model = VGG16_quant()\n",
    "base_model.load_state_dict(orig_state, strict=False)\n",
    "base_model.to(device)\n",
    "base_model.eval()\n",
    "\n",
    "# Save a copy before pruning\n",
    "pruned_model = copy.deepcopy(base_model)\n",
    "pruned_model.train()\n",
    "\n",
    "# Apply 80% unstructured pruning globally\n",
    "target_sparsity = 0.8\n",
    "pruned_model = apply_unstructured_pruning(pruned_model, sparsity=target_sparsity)\n",
    "\n",
    "# compute and print overall sparsity\n",
    "def compute_sparsity(model):\n",
    "    total = 0\n",
    "    zero = 0\n",
    "    for m in model.modules():\n",
    "        if hasattr(m, 'weight') and m.weight is not None:\n",
    "            w = m.weight.data.cpu()\n",
    "            total += w.numel()\n",
    "            zero += (w == 0).sum().item()\n",
    "    return zero / total if total>0 else 0\n",
    "\n",
    "print('Unstructured pruning applied. Sparsity:', compute_sparsity(pruned_model))\n",
    "\n",
    "# Evaluate after pruning (no finetune)\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            out = model(data)\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "    acc = 100. * correct / total\n",
    "    print('Accuracy: {}/{} ({:.2f}%)'.format(correct, total, acc))\n",
    "    return acc\n",
    "\n",
    "baseline_acc = evaluate(base_model, testloader, device)\n",
    "pruned_acc = evaluate(pruned_model, testloader, device)\n",
    "\n",
    "# mark for next step: finetuning\n",
    "print('Baseline (pre-prune) acc:', baseline_acc)\n",
    "print('Post-unstructured-prune acc (before finetune):', pruned_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf82f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Finetune pruned model (unstructured) ---\n",
    "# We'll finetune for a few epochs with a reduced LR to recover accuracy\n",
    "finetune_model = pruned_model\n",
    "finetune_model.train()\n",
    "\n",
    "# create optimizer only for non-zero (masked) params: keep mask params intact\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, finetune_model.parameters()), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "num_epochs = 5  # tweak as needed; short for quick recovery\n",
    "for epoch in range(num_epochs):\n",
    "    print('Finetune epoch', epoch+1, '/', num_epochs)\n",
    "    train(trainloader, finetune_model, criterion, optimizer, epoch)\n",
    "    val_acc = validate(testloader, finetune_model, criterion)\n",
    "    print('Validation acc after epoch:', val_acc)\n",
    "\n",
    "# final evaluation\n",
    "final_acc_unstructured = evaluate(finetune_model, testloader, device)\n",
    "print('Final unstructured-pruned model accuracy after finetune:', final_acc_unstructured)\n",
    "\n",
    "# save checkpoint\n",
    "os.makedirs('result/pruned_unstructured', exist_ok=True)\n",
    "torch.save({'state_dict': finetune_model.state_dict(), 'accuracy': final_acc_unstructured}, 'result/pruned_unstructured/model_best.pth.tar')\n",
    "print('Saved unstructured-pruned model to result/pruned_unstructured/model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337e5ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Structured pruning experiment (channel pruning) ---\n",
    "# Reload a fresh copy of the original checkpoint to start structured pruning from the same baseline\n",
    "struct_model = VGG16_quant()\n",
    "struct_model.load_state_dict(orig_state, strict=False)\n",
    "struct_model.to(device)\n",
    "struct_model.train()\n",
    "\n",
    "# We'll prune entire output channels of Conv layers using L1 unstructured on channel dimension\n",
    "# torch.nn.utils.prune.ln_structured with n=1 and dim=0 prunes output channels\n",
    "def apply_structured_channel_pruning(model, sparsity=0.8):\n",
    "    for m in model.modules():\n",
    "        if m.__class__.__name__.endswith('Conv2d') or m.__class__.__name__ == 'QuantConv2d':\n",
    "            if hasattr(m, 'weight') and m.weight is not None:\n",
    "                # prune output channels (dim=0) by L1 norm\n",
    "                prune.ln_structured(m, name='weight', amount=sparsity, n=1, dim=0)\n",
    "    return model\n",
    "\n",
    "struct_model = apply_structured_channel_pruning(struct_model, sparsity=target_sparsity)\n",
    "print('Structured pruning applied. Channel sparsity (approx):', compute_sparsity(struct_model))\n",
    "\n",
    "# Evaluate before finetune\n",
    "pre_finetune_struct_acc = evaluate(struct_model, testloader, device)\n",
    "print('Post-structured-prune acc (before finetune):', pre_finetune_struct_acc)\n",
    "\n",
    "# Finetune structured-pruned model\n",
    "struct_finetune_optim = optim.SGD(filter(lambda p: p.requires_grad, struct_model.parameters()), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "struct_criterion = nn.CrossEntropyLoss().to(device)\n",
    "num_epochs_struct = 5\n",
    "for epoch in range(num_epochs_struct):\n",
    "    print('Structured finetune epoch', epoch+1, '/', num_epochs_struct)\n",
    "    train(trainloader, struct_model, struct_criterion, struct_finetune_optim, epoch)\n",
    "    va = validate(testloader, struct_model, struct_criterion)\n",
    "    print('Validation acc after epoch:', va)\n",
    "\n",
    "final_acc_structured = evaluate(struct_model, testloader, device)\n",
    "print('Final structured-pruned model accuracy after finetune:', final_acc_structured)\n",
    "\n",
    "os.makedirs('result/pruned_structured', exist_ok=True)\n",
    "torch.save({'state_dict': struct_model.state_dict(), 'accuracy': final_acc_structured}, 'result/pruned_structured/model_best.pth.tar')\n",
    "print('Saved structured-pruned model to result/pruned_structured/model_best.pth.tar')\n",
    "\n",
    "# Summary printout\n",
    "print('SUMMARY:')\n",
    "print('Baseline acc:', baseline_acc)\n",
    "print('After unstructured prune (before finetune):', pruned_acc)\n",
    "print('After unstructured prune + finetune:', final_acc_unstructured)\n",
    "print('After structured prune (before finetune):', pre_finetune_struct_acc)\n",
    "print('After structured prune + finetune:', final_acc_structured)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Prune all the QuantConv2D layers' 90% weights with 1) unstructured, and 2) structured manner.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-owner",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(model.features[40].named_parameters())) # check whether there is mask, weight_org, ...\n",
    "print(model.features[40].weight) # check whether there are many zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-nigeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check sparsity ###\n",
    "mask1 = model.features[40].weight_mask\n",
    "sparsity_mask1 = (mask1 == 0).sum() / mask1.nelement()\n",
    "\n",
    "print(\"Sparsity level: \", sparsity_mask1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-vampire",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check accuracy after pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-worst",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start finetuning (training here), and see how much you can recover your accuracy ##\n",
    "## You can change hyper parameters such as epochs or lr ##\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-ready",
   "metadata": {},
   "outputs": [],
   "source": [
    "## check your accuracy again after finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-tanzania",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Send an image and use prehook to grab the inputs of all the QuantConv2d layers\n",
    "\n",
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "    def __call__(self, module, module_in):\n",
    "        self.outputs.append(module_in)\n",
    "    def clear(self):\n",
    "        self.outputs = []  \n",
    "        \n",
    "######### Save inputs from selected layer ##########\n",
    "save_output = SaveOutput()\n",
    "i = 0\n",
    "\n",
    "for layer in model.modules():\n",
    "    i = i+1\n",
    "    if isinstance(layer, QuantConv2d):\n",
    "        print(i,\"-th layer prehooked\")\n",
    "        layer.register_forward_pre_hook(save_output)             \n",
    "####################################################\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.to(device)\n",
    "out = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-peeing",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Find \"weight_int\" for features[3] ####\n",
    "w_bit = 4\n",
    "weight_q = model.features[3].weight_q\n",
    "w_alpha = model.features[3].weight_quant.wgt_alpha\n",
    "w_delta = w_alpha /(2**(w_bit-1)-1)\n",
    "\n",
    "weight_int = weight_q / w_delta\n",
    "print(weight_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-firmware",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### check your sparsity for weight_int is near 90% #####\n",
    "#### Your sparsity could be >90% after quantization #####\n",
    "sparsity_weight_int = (weight_int == 0).sum() / weight_int.nelement()\n",
    "print(\"Sparsity level: \", sparsity_weight_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-cancer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-excuse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-auction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-niger",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-whole",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-significance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-witch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-barbados",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-serbia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
